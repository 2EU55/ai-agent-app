import os

import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
import re

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
DEFAULT_DOC_PATH = "company_policy.txt"
DEFAULT_EMBEDDING_MODEL = "BAAI/bge-m3"
DEFAULT_CHAT_MODEL = "THUDM/glm-4-9b-chat"


def resolve_doc_path(raw_path: str) -> tuple[str | None, list[str]]:
    p = (raw_path or "").strip().strip('"').strip("'")
    p = os.path.normpath(p)
    if not p:
        return None, []

    base_dir = os.path.dirname(os.path.abspath(__file__))
    candidates: list[str] = []

    if os.path.isabs(p):
        candidates.append(p)
    else:
        candidates.append(os.path.join(base_dir, p))
        candidates.append(os.path.join(os.getcwd(), p))

        parts = p.replace("/", "\\").split("\\")
        if parts and parts[0].lower() == "ai_intern_bootcamp":
            candidates.append(os.path.join(base_dir, *parts[1:]))

    seen: set[str] = set()
    uniq: list[str] = []
    for c in candidates:
        if c not in seen:
            seen.add(c)
            uniq.append(c)

    for c in uniq:
        if os.path.exists(c):
            return c, uniq
    return None, uniq


def detect_missing_fields(question: str, context: str) -> str | None:
    q = (question or "").strip()
    if not q:
        return None
    ctx = context or ""

    month_count_q = re.search(r"(å‡ |å¤šå°‘)\s*ä¸ª?\s*æœˆ", q)
    if month_count_q:
        if not re.search(r"(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*ä¸ª?\s*æœˆ", ctx):
            return "â€œå‡ ä¸ªæœˆï¼ˆæ•°é‡ï¼‰â€"

    money_q = any(k in q for k in ["å¤šå°‘é’±", "å¤šå°‘å…ƒ", "é‡‘é¢", "è´¹ç”¨", "æŠ¥é”€", "è¡¥è´´", "æ ‡å‡†"])
    if money_q:
        if not (re.search(r"\d+", ctx) and re.search(r"(å…ƒ|ä¸‡)", ctx)):
            return "â€œé‡‘é¢/æ ‡å‡†â€"

    return None


def load_env() -> None:
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except Exception:
        return



def get_default_api_key() -> str:
    return os.getenv('SILICONFLOW_API_KEY','')


def build_embeddings(api_key: str, embedding_model: str) -> OpenAIEmbeddings:
    return OpenAIEmbeddings(
        model = embedding_model,
        api_key = api_key,
        base_url = SILICONFLOW_BASE_URL,
    )


def build_llm(api_key: str, chat_model: str) -> ChatOpenAI:
    return ChatOpenAI(
        model = chat_model,
        api_key = api_key,
        base_url = SILICONFLOW_BASE_URL,
        temperature = 0,
    )


def build_prompt() -> ChatPromptTemplate:
    template = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "ä½ å¿…é¡»åªåŸºäºä¸Šä¸‹æ–‡ä½œç­”ã€‚\n"
        "å¦‚æœä½ æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¤åˆ¶å‡ºä¸€å¥èƒ½æ”¯æ’‘ç­”æ¡ˆçš„åŸæ–‡å¥å­ï¼Œå°±è¾“å‡ºâ€œä¸çŸ¥é“â€ï¼Œä¸è¦çç¼–ã€‚\n"
        "ä½ å¿…é¡»éµå®ˆä»¥ä¸‹è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰ï¼š\n"
        "1) ç­”æ¡ˆï¼š...\n"
        "2) å¼•ç”¨ï¼šåˆ—å‡ºä½ ä½¿ç”¨åˆ°çš„ç‰‡æ®µç¼–å·ï¼Œä¾‹å¦‚ï¼š[ç‰‡æ®µ1] [ç‰‡æ®µ3]ï¼›å¦‚æœæ²¡æœ‰ä¾æ®ï¼Œå†™ï¼šæ— \n"
        "3) è¯æ®åŸæ–‡ï¼šä»å¼•ç”¨ç‰‡æ®µä¸­å¤åˆ¶ 1 å¥åŸæ–‡ï¼ˆæ‰¾ä¸åˆ°å°±å†™ï¼šæ— ï¼‰\n\n"
        "ä¸Šä¸‹æ–‡ï¼ˆä½ åªèƒ½å¼•ç”¨è¿™é‡Œå‡ºç°çš„ç‰‡æ®µç¼–å·ï¼‰ï¼š\n{context}\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š\n{question}\n"
    )
    return ChatPromptTemplate.from_template(template)


@st.cache_resource
def build_vectorstore(
    api_key: str,
    doc_path: str,
    embedding_model: str,
    chunk_size: int,
    chunk_overlap: int,
) -> Chroma:
    doc_path_abs, _ = resolve_doc_path(doc_path)
    if not doc_path_abs:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡æ¡£ï¼š{doc_path}")

    loader = TextLoader(doc_path_abs, encoding="utf-8")
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)
    splits = splitter.split_documents(docs)

    embeddings = build_embeddings(api_key=api_key,embedding_model=embedding_model)

    return Chroma.from_documents(documents=splits,embedding=embeddings)


def format_docs(docs) -> str:
    parts: list[str] = []
    for i, d in enumerate(docs, start=1):
        content = (d.page_content or "").strip()
        if not content:
            continue
        parts.append(f"[ç‰‡æ®µ{i}]\n{content}")
    return "\n\n".join(parts).strip()


def render_page() -> None:
    st.set_page_config(page_title="handwrite", page_icon="ğŸ“š")
    st.title('æ‰‹å†™ä»£ç è®­ç»ƒ')

    with st.sidebar:
        api_key = st.text_input('api_key',value=get_default_api_key(),type='password')
        doc_path = st.text_input('doc_path',value=DEFAULT_DOC_PATH)
        embedding_model = st.text_input('embedding_model',value=DEFAULT_EMBEDDING_MODEL)
        chat_model = st.text_input('chat_model',value=DEFAULT_CHAT_MODEL)

        chunk_size = st.slider('chunk_size',100,800,200,50)
        chunk_overlap = st.slider('chunk_overlap',0,200,50,10)
        top_k = st.slider('Top-K',1,10,4,1)

    if not api_key:
        st.info('è¯·è¾“å…¥API_KEY')
        return

    doc_path_abs, attempted = resolve_doc_path(doc_path)
    if not doc_path_abs:
        st.error("æ‰¾ä¸åˆ°æ–‡æ¡£ï¼Œå°è¯•è¿‡è¿™äº›è·¯å¾„ï¼š\n" + "\n".join(attempted))
        return
    st.sidebar.caption(f"å®é™…åŠ è½½æ–‡æ¡£ï¼š{doc_path_abs}")

    if 'messages' not in st.session_state:
        st.session_state.messages = []

    for m in st.session_state.messages:
        with st.chat_message(m['role']):
            st.markdown(m['content'])

    question = st.chat_input('é—®ç‚¹ä»€ä¹ˆ')
    if not question:
        return

    st.session_state.messages.append({'role':'user','content':question})
    with st.chat_message('user'):
        st.markdown(question)

    vectorstore = build_vectorstore(
        api_key=api_key,
        doc_path=doc_path_abs,
        embedding_model=embedding_model,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )

    retriever = vectorstore.as_retriever(search_kwargs={'k':top_k})

    docs = retriever.invoke(question) if hasattr(retriever, "invoke") else retriever.get_relevant_documents(question)
    context = format_docs(docs)

    if not context.strip():
        answer = "1) ç­”æ¡ˆï¼šä¸çŸ¥é“\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "
    else:
        missing = detect_missing_fields(question=question, context=context)
        if missing:
            answer = f"1) ç­”æ¡ˆï¼šä¸çŸ¥é“ï¼ˆæ–‡æ¡£æœªæä¾›{missing}ï¼‰\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "
        else:
            prompt = build_prompt()
            llm = build_llm(api_key=api_key, chat_model=chat_model)
            resp = llm.invoke(prompt.format_messages(context=context,question=question))
            answer = (getattr(resp,'content','')or'').strip() or "1) ç­”æ¡ˆï¼šä¸çŸ¥é“\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "

    st.session_state.messages.append({'role':'assistant','content':answer})
    with st.chat_message('assistant'):
        st.markdown(answer)
        with st.expander('æœ¬æ¬¡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ’é”™ï¼‰', expanded=False):
            st.text(context or '(ç©º)')


load_env()
render_page()
