import os

import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
DEFAULT_DOC_PATH = "company_policy.txt"
DEFAULT_EMBEDDING_MODEL = "BAAI/bge-m3"
DEFAULT_CHAT_MODEL = "THUDM/glm-4-9b-chat"


def load_env() -> None:
    try:
        from dotenv import load_dotenv

        load_dotenv()
    except Exception:
        return


def get_default_api_key() -> str:
    return os.getenv("SILICONFLOW_API_KEY", "")


def build_embeddings(api_key: str, embedding_model: str) -> OpenAIEmbeddings:
    return OpenAIEmbeddings(
        model=embedding_model,
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
    )


def build_llm(api_key: str, chat_model: str) -> ChatOpenAI:
    return ChatOpenAI(
        model=chat_model,
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
        temperature=0,
    )


def build_prompt() -> ChatPromptTemplate:
    template = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "å¦‚æœä½ åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œå°±è€å®è¯´ä¸çŸ¥é“ï¼Œä¸è¦çç¼–ã€‚\n"
        "ä½ å¿…é¡»éµå®ˆä»¥ä¸‹è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰ï¼š\n"
        "1) ç­”æ¡ˆï¼š...\n"
        "2) å¼•ç”¨ï¼šåˆ—å‡ºä½ ä½¿ç”¨åˆ°çš„ç‰‡æ®µç¼–å·ï¼Œä¾‹å¦‚ï¼š[ç‰‡æ®µ1] [ç‰‡æ®µ3]ï¼›å¦‚æœæ²¡æœ‰ä¾æ®ï¼Œå†™ï¼šæ— \n\n"
        "ä¸Šä¸‹æ–‡ï¼ˆä½ åªèƒ½å¼•ç”¨è¿™é‡Œå‡ºç°çš„ç‰‡æ®µç¼–å·ï¼‰ï¼š\n{context}\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š\n{question}\n"
    )
    return ChatPromptTemplate.from_template(template)


@st.cache_resource
def build_vectorstore(
    api_key: str,
    doc_path: str,
    embedding_model: str,
    chunk_size: int,
    chunk_overlap: int,
) -> Chroma:
    loader = TextLoader(doc_path, encoding="utf-8")
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    splits = splitter.split_documents(docs)

    embeddings = build_embeddings(api_key=api_key, embedding_model=embedding_model)
    return Chroma.from_documents(documents=splits, embedding=embeddings)


def format_docs(docs) -> str:
    parts: list[str] = []
    for i, d in enumerate(docs, start=1):
        content = (d.page_content or "").strip()
        source = d.metadata.get("source", "æœªçŸ¥æ¥æº")
        if not content:
            continue
        parts.append(f"[ç‰‡æ®µ{i}] (æ¥æº: {source})\n{content}")
    return "\n\n".join(parts).strip()


def render_page() -> None:
    st.set_page_config(page_title="RAG Minimal", page_icon="ğŸ“š")
    st.title("ğŸ“š RAG Minimalï¼ˆå¯å¤åˆ»ç‰ˆï¼‰")

    with st.sidebar:
        api_key = st.text_input("API Key", value=get_default_api_key(), type="password")
        doc_path = st.text_input("æ–‡æ¡£è·¯å¾„", value=DEFAULT_DOC_PATH)
        chat_model = st.text_input("Chat æ¨¡å‹", value=DEFAULT_CHAT_MODEL)
        embedding_model = st.text_input("Embedding æ¨¡å‹", value=DEFAULT_EMBEDDING_MODEL)
        chunk_size = st.slider("chunk_size", min_value=100, max_value=800, value=200, step=50)
        chunk_overlap = st.slider("chunk_overlap", min_value=0, max_value=200, value=50, step=10)
        top_k = st.slider("Top-K", min_value=1, max_value=10, value=4, step=1)
        refuse_when_empty = st.checkbox("æ£€ç´¢ä¸ºç©ºæ—¶ç›´æ¥æ‹’ç­”", value=True)
        st.divider()
        st.caption(f"Base URLï¼š{SILICONFLOW_BASE_URL}")

    if not api_key:
        st.info("å…ˆåœ¨å·¦ä¾§é…ç½® API Keyï¼ˆæ”¯æŒä» .env è‡ªåŠ¨è¯»å–ï¼‰ã€‚")
        return

    if not os.path.exists(doc_path):
        st.error(f"æ‰¾ä¸åˆ°æ–‡æ¡£ï¼š{doc_path}")
        return

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    question = st.chat_input("è¯•ç€é—®é—®ï¼šå‡ºå·®ä½å®¿æ ‡å‡†æ˜¯å¤šå°‘ï¼Ÿ")
    if not question:
        return

    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    try:
        vectorstore = build_vectorstore(
            api_key=api_key,
            doc_path=doc_path,
            embedding_model=embedding_model,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
        docs = retriever.invoke(question) if hasattr(retriever, "invoke") else retriever.get_relevant_documents(question)
        context = format_docs(docs)

        if refuse_when_empty and not context.strip():
            answer = "1) ç­”æ¡ˆï¼šæˆ‘åœ¨èµ„æ–™åº“é‡Œæ²¡æœ‰æ‰¾åˆ°ä¾æ®ï¼Œæ‰€ä»¥æˆ‘ä¸çŸ¥é“ã€‚ä½ å¯ä»¥æ¢ä¸ªé—®æ³•æˆ–æä¾›æ›´å¤šç»†èŠ‚ã€‚\n2) å¼•ç”¨ï¼šæ— "
        else:
            prompt = build_prompt()
            llm = build_llm(api_key=api_key, chat_model=chat_model)
            messages = prompt.format_messages(context=context, question=question)
            resp = llm.invoke(messages)
            answer = (getattr(resp, "content", None) or "").strip()
            if not answer:
                answer = "1) ç­”æ¡ˆï¼šæˆ‘æš‚æ—¶æ²¡ç”Ÿæˆå‡ºæœ‰æ•ˆå›ç­”ã€‚\n2) å¼•ç”¨ï¼šæ— "

        with st.chat_message("assistant"):
            st.markdown(answer)

        st.session_state.messages.append({"role": "assistant", "content": answer})

        with st.expander("æœ¬æ¬¡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç†è§£ RAGï¼‰", expanded=False):
            st.text(context or "(ç©º)")
    except Exception as e:
        st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")


load_env()
render_page()

