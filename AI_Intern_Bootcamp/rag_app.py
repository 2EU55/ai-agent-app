import streamlit as st  # ç½‘é¡µç•Œé¢åº“
import os
import re
import hashlib
import shutil
# --- LangChain æ ¸å¿ƒç»„ä»¶ ---
from langchain_core.documents import Document  # ç”¨äºæ‰‹åŠ¨æ„å»ºæ–‡æ¡£å¯¹è±¡
from langchain_text_splitters import RecursiveCharacterTextSplitter  # ç”¨äºæŠŠé•¿æ–‡ç« åˆ‡æˆå°å—
from langchain_community.vectorstores import Chroma  # å‘é‡æ•°æ®åº“ï¼ˆæœ¬åœ°æ–‡ä»¶ç‰ˆï¼‰
from langchain_community.embeddings import OpenAIEmbeddings  # å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡ï¼ˆæ•°å­—åˆ—è¡¨ï¼‰
from langchain_core.prompts import ChatPromptTemplate  # æç¤ºè¯æ¨¡æ¿
from langchain_community.chat_models import ChatOpenAI  # è°ƒç”¨å¤§æ¨¡å‹ (LLM)

# --- è‡ªå®šä¹‰è§„åˆ™ (æˆ‘ä»¬è‡ªå·±å†™çš„ Python æ–‡ä»¶) ---
from refusal_rules import detect_missing_fields, extract_query_terms, should_refuse_by_score, term_overlap_hits

# --- é…ç½®é¡¹ (Configuration) ---
# ç¡…åŸºæµåŠ¨ API åœ°å€ (å…¼å®¹ OpenAI æ ¼å¼)
SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
DEFAULT_DOC_PATH = "company_policy.txt"  # é»˜è®¤è¯»å–çš„çŸ¥è¯†åº“æ–‡ä»¶
EMBEDDING_MODEL = "BAAI/bge-m3"  # åµŒå…¥æ¨¡å‹ï¼šè´Ÿè´£æŠŠæ–‡å­—å˜æˆæ•°å­—
# CHAT_MODEL = "THUDM/glm-4-9b-chat"  # æ—§æ¨¡å‹
CHAT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # æ–°æ¨¡å‹ï¼šæ”¯æŒ Tool Calling
PERSIST_ROOT_DIRNAME = ".chroma_rag"  # å‘é‡æ•°æ®åº“å­˜æ”¾åœ¨å“ªï¼ˆç¼“å­˜ç›®å½•ï¼‰
RETRIEVAL_TOP_K = 4  # æ¯æ¬¡æ£€ç´¢æ‰¾å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç‰‡æ®µï¼Ÿ
SCORE_THRESHOLD = 0.65  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½äºè¿™ä¸ªåˆ†æ•°çš„è®¤ä¸ºä¸ç›¸å…³ï¼‰
TERM_OVERLAP_MIN_HITS = 1  # å…³é”®è¯å‘½ä¸­æ•°ï¼ˆè‡³å°‘å‘½ä¸­å‡ ä¸ªè¯æ‰ç®—ç›¸å…³ï¼Ÿï¼‰


# --- æ ¸å¿ƒå‡½æ•° (Core Functions) ---

def resolve_doc_path(raw_path: str) -> tuple[str | None, list[str]]:
    """
    [è¾…åŠ©å‡½æ•°] å¤„ç†æ–‡ä»¶è·¯å¾„ã€‚
    ä¸ç®¡æ˜¯ç›¸å¯¹è·¯å¾„è¿˜æ˜¯ç»å¯¹è·¯å¾„ï¼Œéƒ½è¯•ç€æ‰¾ä¸€æ‰¾ï¼Œé˜²æ­¢æŠ¥é”™ã€‚
    """
    p = (raw_path or "").strip().strip('"').strip("'")

    p = os.path.normpath(p)
    if not p:
        return None, []

    base_dir = os.path.dirname(os.path.abspath(__file__))
    candidates: list[str] = []

    if os.path.isabs(p):
        candidates.append(p)
    else:
        candidates.append(os.path.join(base_dir, p))
        candidates.append(os.path.join(os.getcwd(), p))

        parts = p.replace("/", "\\").split("\\")
        if parts and parts[0].lower() == "ai_intern_bootcamp":
            candidates.append(os.path.join(base_dir, *parts[1:]))

    seen: set[str] = set()
    uniq: list[str] = []
    for c in candidates:
        if c not in seen:
            seen.add(c)
            uniq.append(c)

    for c in uniq:
        if os.path.exists(c):
            return c, uniq
    return None, uniq


def file_fingerprint(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def load_env() -> None:
    try:
        from dotenv import load_dotenv
        base_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.abspath(os.path.join(base_dir, os.pardir))
        dotenv_candidates = [
            os.path.join(base_dir, ".env"),
            os.path.join(root_dir, ".env"),
            os.path.join(root_dir, "AI_Intern_Bootcamp", ".env"),
            os.path.join(os.getcwd(), ".env"),
        ]
        for p in dotenv_candidates:
            if os.path.exists(p):
                load_dotenv(dotenv_path=p, override=False)
                break
    except Exception:
        return


def get_default_api_key() -> str:
    return os.getenv("SILICONFLOW_API_KEY", "")


def build_embeddings(api_key: str) -> OpenAIEmbeddings:
    """
    åˆå§‹åŒ– Embedding æ¨¡å‹ã€‚
    Embedding æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆå‘é‡çš„å·¥å…·ã€‚
    æ¯”å¦‚ "è‹¹æœ" -> [0.1, 0.2, 0.9]
    """
    return OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
        timeout=30,
        max_retries=0,
    )


def build_llm(api_key: str) -> ChatOpenAI:
    """
    åˆå§‹åŒ–å¤§æ¨¡å‹ (LLM)ã€‚
    temperature=0 è¡¨ç¤ºæˆ‘ä»¬è¦å®ƒä¸¥è°¨ä¸€ç‚¹ï¼Œä¸è¦è‡ªç”±å‘æŒ¥ã€‚
    """
    return ChatOpenAI(
        model=CHAT_MODEL,
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
        temperature=0,
        timeout=30,
        max_retries=0,
    )


def retrieve_docs(retriever, question: str):
    """
    [ç®€å•æ£€ç´¢] ç»™å®ƒä¸€ä¸ªé—®é¢˜ï¼Œå®ƒè¿”å›ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    """
    if hasattr(retriever, "invoke"):
        docs = retriever.invoke(question)
    else:
        docs = retriever.get_relevant_documents(question)
    return docs or []


def retrieve_docs_with_scores(retriever, question: str, k: int):
    """
    [å¸¦åˆ†æ•°çš„æ£€ç´¢] ä¸ä»…è¿”å›æ–‡æ¡£ï¼Œè¿˜è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
    åˆ†æ•°è¶Šé«˜ï¼Œè¯´æ˜è¿™æ®µè¯å’Œé—®é¢˜è¶Šç›¸å…³ã€‚
    """
    vectorstore = getattr(retriever, "vectorstore", None)
    if vectorstore is not None:
        if hasattr(vectorstore, "similarity_search_with_relevance_scores"):
            docs_and_scores = vectorstore.similarity_search_with_relevance_scores(question, k=k)
            docs = [d for d, _ in docs_and_scores]
            scores = [float(s) for _, s in docs_and_scores]
            return docs, scores, "relevance"
        if hasattr(vectorstore, "similarity_search_with_score"):
            docs_and_scores = vectorstore.similarity_search_with_score(question, k=k)
            docs = [d for d, _ in docs_and_scores]
            scores = [float(s) for _, s in docs_and_scores]
            return docs, scores, "distance"

    docs = retrieve_docs(retriever, question)
    return docs, [], "unknown"


def format_docs_with_ids(docs) -> str:
    parts: list[str] = []
    for i, d in enumerate(docs, start=1):
        text = (getattr(d, "page_content", "") or "").strip()
        if not text:
            continue
        parts.append(f"[ç‰‡æ®µ{i}]\n{text}")
    return "\n\n".join(parts).strip()


def pick_evidence_from_chunk(text: str) -> str:
    t = (text or "").strip()
    if not t:
        return "æ— "
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    candidates: list[str] = []
    for ln in lines:
        cleaned = ln.lstrip("- ").strip()
        if not cleaned:
            continue
        if cleaned.startswith("#"):
            continue
        if len(cleaned) < 8:
            continue
        candidates.append(cleaned)

    if not candidates:
        return "æ— "

    def score(s: str) -> int:
        sc = 0
        if re.search(r"(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*(?:å…ƒ|å¤©|æœˆ|%|å¹´|æ—¥|å‘¨)", s):
            sc += 4
        if re.search(r"(ä½å®¿|æ ‡å‡†|ä¸è¶…è¿‡|æŠ¥é”€|è¡¥è´´|å¹´ç»ˆå¥–|å‘æ”¾|å¹´å‡|ç—…å‡|å·¥èµ„|äº¤é€š)", s):
            sc += 3
        if ("ï¼›" in s) or ("ã€‚" in s) or ("ï¼š" in s):
            sc += 1
        return sc

    best = max(candidates, key=score)
    return best


def fill_evidence_if_missing(response: str, docs) -> str:
    if not response:
        return response

    def norm(s: str) -> str:
        t = (s or "").strip()
        if not t:
            return ""
        t = re.sub(r"[\s\u3000]+", "", t)
        t = re.sub(r"[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€ï¼ˆï¼‰()ã€ã€‘\\[\\]â€œâ€\"'ã€Šã€‹<>Â·]", "", t)
        return t

    def evidence_in_chunk(ev: str, chunk: str) -> bool:
        if not ev or ev == "æ— ":
            return False
        if ev in chunk:
            return True
        nev = norm(ev)
        if not nev:
            return False
        return nev in norm(chunk)

    cite_ids = [int(x) for x in re.findall(r"\[ç‰‡æ®µ(\d+)\]", response)]
    cite_ids = [i for i in cite_ids if i > 0]
    cite_ids = list(dict.fromkeys(cite_ids))
    if not cite_ids:
        return response

    m_ans = re.search(r"ç­”æ¡ˆï¼š\s*(.*)", response)
    answer_text = (m_ans.group(1) if m_ans else "").strip()
    is_refusal = (not answer_text) or ("ä¸çŸ¥é“" in answer_text)

    m_ev = re.search(r"è¯æ®åŸæ–‡ï¼š\s*(.*)", response)
    if not m_ev:
        return response
    evidence_text = (m_ev.group(1) or "").strip()

    found_in_idx: int | None = None
    for idx, d in enumerate(docs):
        chunk = (getattr(d, "page_content", "") or "").strip()
        if evidence_in_chunk(evidence_text, chunk):
            found_in_idx = idx
            break

    cited_supported = False
    for cid in cite_ids:
        idx = cid - 1
        if idx < 0 or idx >= len(docs):
            continue
        chunk = (getattr(docs[idx], "page_content", "") or "").strip()
        if evidence_in_chunk(evidence_text, chunk):
            cited_supported = True
            break

    if (not is_refusal) and ((not evidence_text) or evidence_text == "æ— "):
        for cid in cite_ids:
            idx = cid - 1
            if idx < 0 or idx >= len(docs):
                continue
            chunk = (getattr(docs[idx], "page_content", "") or "").strip()
            candidate = pick_evidence_from_chunk(chunk)
            if candidate and candidate != "æ— ":
                evidence_text = candidate
                break

    if (not is_refusal) and (not cited_supported):
        if found_in_idx is not None:
            new_cite = f"[ç‰‡æ®µ{found_in_idx + 1}]"
            response = re.sub(r"(å¼•ç”¨ï¼š).*", f"\\1 {new_cite}", response)
        else:
            repaired_cid: int | None = None
            repaired_ev: str = ""
            for cid in cite_ids:
                idx = cid - 1
                if idx < 0 or idx >= len(docs):
                    continue
                chunk = (getattr(docs[idx], "page_content", "") or "").strip()
                repaired_ev = pick_evidence_from_chunk(chunk)
                if repaired_ev and repaired_ev != "æ— ":
                    repaired_cid = cid
                    break
            if repaired_cid is None:
                response = re.sub(r"ç­”æ¡ˆï¼š.*", "ç­”æ¡ˆï¼šä¸çŸ¥é“ï¼ˆè¯æ®ä¸è¶³ï¼‰", response)
                response = re.sub(r"(å¼•ç”¨ï¼š).*", "å¼•ç”¨ï¼šæ— ", response)
                response = re.sub(r"è¯æ®åŸæ–‡ï¼š.*", "è¯æ®åŸæ–‡ï¼šæ— ", response)
                return response
            response = re.sub(r"(å¼•ç”¨ï¼š).*", f"\\1 [ç‰‡æ®µ{repaired_cid}]", response)
            evidence_text = repaired_ev

    if evidence_text and evidence_text != (m_ev.group(1) or "").strip():
        response = re.sub(r"è¯æ®åŸæ–‡ï¼š\s*.*", f"è¯æ®åŸæ–‡ï¼š{evidence_text}", response)

    return response


def build_prompt() -> ChatPromptTemplate:
    template = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "ä½ å¿…é¡»åªåŸºäºä¸Šä¸‹æ–‡ä½œç­”ã€‚\n"
        "å¦‚æœä½ æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¤åˆ¶å‡ºä¸€å¥èƒ½æ”¯æ’‘ç­”æ¡ˆçš„åŸæ–‡å¥å­ï¼Œå°±è¾“å‡ºâ€œä¸çŸ¥é“â€ï¼Œä¸è¦çç¼–ã€‚\n"
        "è¯æ®åŸæ–‡å¿…é¡»é€å­—å¤åˆ¶è‡ªå¼•ç”¨ç‰‡æ®µä¸­çš„è¿ç»­æ–‡æœ¬ï¼ˆä¸å¾—æ”¹å†™ã€ä¸å¾—è‡ªè¡Œè¡¥æ ‡ç‚¹/ç©ºæ ¼ï¼‰ã€‚\n"
        "è¯æ®åŸæ–‡åªèƒ½å¤åˆ¶ 1 å¥ï¼Œä¸å¾—æŠŠå¤šå¥æ‹¼æ¥æˆä¸€è¡Œï¼›ä¸å¾—æ·»åŠ æ‹¬å·è§£é‡Šã€‚\n\n"
        "è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\n"
        "1) ç­”æ¡ˆï¼š...\n"
        "2) å¼•ç”¨ï¼šåˆ—å‡ºä½ ä½¿ç”¨åˆ°çš„ç‰‡æ®µç¼–å·ï¼Œä¾‹å¦‚ï¼š[ç‰‡æ®µ1] [ç‰‡æ®µ3]ï¼›å¦‚æœæ²¡æœ‰ä¾æ®ï¼Œå†™ï¼šæ— \n"
        "3) è¯æ®åŸæ–‡ï¼šä»å¼•ç”¨ç‰‡æ®µä¸­å¤åˆ¶ 1 å¥åŸæ–‡ï¼ˆæ‰¾ä¸åˆ°å°±å†™ï¼šæ— ï¼‰\n\n"
        "ä¸Šä¸‹æ–‡ï¼ˆä½ åªèƒ½å¼•ç”¨è¿™é‡Œå‡ºç°çš„ç‰‡æ®µç¼–å·ï¼‰ï¼š\n{context}\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š\n{question}\n"
    )
    return ChatPromptTemplate.from_template(template)


@st.cache_resource
def build_retriever(api_key: str):
    """
    [æ ¸å¿ƒå‡½æ•°] æ„å»ºæ£€ç´¢å™¨ (Retriever)ã€‚
    å®ƒçš„å·¥ä½œæµç¨‹ï¼š
    1. æ£€æŸ¥æœ‰æ²¡æœ‰ç°æˆçš„å‘é‡åº“ (.chroma_rag æ–‡ä»¶å¤¹)ã€‚
    2. å¦‚æœæœ‰ï¼Œç›´æ¥åŠ è½½ï¼ˆçœæ—¶é—´ï¼‰ã€‚
    3. å¦‚æœæ²¡æœ‰ï¼Œè¯»å– txt -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å…¥æ•°æ®åº“ã€‚
    """
    if not api_key:
        return None

    doc_path_abs, _ = resolve_doc_path(DEFAULT_DOC_PATH)
    if not doc_path_abs:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡æ¡£ï¼š{DEFAULT_DOC_PATH}")

    embeddings = build_embeddings(api_key=api_key)

    # è®¡ç®—æ–‡ä»¶çš„æŒ‡çº¹ï¼ˆå¦‚æœæ–‡ä»¶å†…å®¹å˜äº†ï¼ŒæŒ‡çº¹å°±ä¼šå˜ï¼Œæˆ‘ä»¬å°±ä¼šé‡æ–°å»ºç«‹ç´¢å¼•ï¼‰
    fingerprint = file_fingerprint(doc_path_abs)
    persist_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), PERSIST_ROOT_DIRNAME)
    persist_dir = os.path.join(persist_root, fingerprint)

    if os.path.exists(persist_dir):
        # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
        vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
        return vectorstore.as_retriever()

    # --- å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹ä»å¤´æ„å»º ---
    
    # 1. è¯»å–æ–‡ä»¶
    with open(doc_path_abs, "r", encoding="utf-8") as f:
        text = f.read()
    docs = [Document(page_content=text, metadata={"source": doc_path_abs})]

    # 2. åˆ‡åˆ†æ–‡æ¡£ (Chunking)
    # chunk_size=200: æ¯å—çº¦ 200 ä¸ªå­—
    # chunk_overlap=50: æ¯å—ä¹‹é—´é‡å  50 ä¸ªå­—ï¼ˆé˜²æ­¢å¥å­è¢«åˆ‡æ–­ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    # 3. å­˜å…¥å‘é‡æ•°æ®åº“
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=persist_dir,
    )
    if hasattr(vectorstore, "persist"):
        vectorstore.persist()
    return vectorstore.as_retriever()


def render_page() -> None:
    st.set_page_config(page_title="ä¼ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ (RAG Demo)", page_icon="ğŸ“š")
    st.title("ğŸ“š ä¼ä¸šç§æœ‰çŸ¥è¯†åº“åŠ©æ‰‹")
    st.markdown("è¿™æ˜¯ä¸€ä¸ª **RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)** çš„å®æˆ˜æ¼”ç¤ºã€‚AI åŸºäºæˆ‘ä»¬æä¾›çš„ã€Šå‘˜å·¥æ‰‹å†Œã€‹å›ç­”é—®é¢˜ã€‚")

    with st.sidebar:
        st.header("é…ç½®")
        api_key = st.text_input("API Key", value=get_default_api_key(), type="password")
        learning_mode = st.checkbox("å­¦ä¹ æ¨¡å¼ï¼ˆå±•ç¤ºè°ƒè¯•ä¿¡æ¯ï¼‰", value=True)
        st.divider()
        st.write(f"Base URLï¼š{SILICONFLOW_BASE_URL}")
        doc_path_abs, attempted = resolve_doc_path(DEFAULT_DOC_PATH)
        if doc_path_abs:
            st.write(f"æ–‡æ¡£ï¼š{DEFAULT_DOC_PATH}")
            st.caption(f"å®é™…åŠ è½½ï¼š{doc_path_abs}")
        else:
            st.error("æ–‡æ¡£è·¯å¾„è§£æå¤±è´¥ï¼š\n" + "\n".join(attempted))
        st.write(f"Embeddingï¼š{EMBEDDING_MODEL}")
        st.write(f"Chatï¼š{CHAT_MODEL}")
        if learning_mode:
            st.divider()
            st.subheader("å­¦ä¹ å‚æ•°")
            top_k = st.number_input("top_k", min_value=1, max_value=10, value=int(RETRIEVAL_TOP_K), step=1)
            score_threshold = st.slider("score_threshold", min_value=0.0, max_value=1.0, value=float(SCORE_THRESHOLD), step=0.01)
            term_overlap_min_hits = st.number_input(
                "term_overlap_min_hits", min_value=0, max_value=5, value=int(TERM_OVERLAP_MIN_HITS), step=1
            )
        else:
            top_k = RETRIEVAL_TOP_K
            score_threshold = SCORE_THRESHOLD
            term_overlap_min_hits = TERM_OVERLAP_MIN_HITS
        st.divider()
        st.caption(f"ç´¢å¼•ç¼“å­˜ç›®å½•ï¼š{os.path.join(os.path.dirname(os.path.abspath(__file__)), PERSIST_ROOT_DIRNAME)}")
        if st.button("é‡å»ºç´¢å¼•ï¼ˆæ¸…ç©ºç¼“å­˜ï¼‰", use_container_width=True):
            persist_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), PERSIST_ROOT_DIRNAME)
            if os.path.exists(persist_root):
                shutil.rmtree(persist_root, ignore_errors=True)
            try:
                st.cache_resource.clear()
            except Exception:
                pass
            st.rerun()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è¯•ç€é—®é—®ï¼šå‡ºå·®ä½å®¿æ ‡å‡†æ˜¯å¤šå°‘ï¼Ÿ"):
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        if not api_key:
            st.error("è¯·å…ˆé…ç½® API Keyï¼ˆå·²æ”¯æŒä» .env è‡ªåŠ¨è¯»å–ï¼‰ã€‚")
            return

        retriever = build_retriever(api_key=api_key)
        if not retriever:
            st.error("æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥ã€‚")
            return

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("æ£€ç´¢ä¸­â€¦")

            try:
                llm = build_llm(api_key=api_key)
                prompt_template = build_prompt()

                docs, scores, score_mode = retrieve_docs_with_scores(retriever, prompt, k=int(top_k))
                best_score = scores[0] if scores else None
                context = format_docs_with_ids(docs)
                hits = term_overlap_hits(prompt, context)
                missing = detect_missing_fields(prompt, context)
                topic_missing = len(hits) < int(term_overlap_min_hits)
                refuse_by_score = topic_missing and should_refuse_by_score(score_mode, best_score, float(score_threshold))

                if not context.strip():
                    response = "1) ç­”æ¡ˆï¼šä¸çŸ¥é“\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "
                else:
                    if missing:
                        response = f"1) ç­”æ¡ˆï¼šä¸çŸ¥é“ï¼ˆæ–‡æ¡£æœªæä¾›{missing}ï¼‰\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "
                    elif refuse_by_score:
                        response = "1) ç­”æ¡ˆï¼šä¸çŸ¥é“ï¼ˆè¯æ®ä¸è¶³ï¼‰\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "
                    else:
                        messages = prompt_template.format_messages(context=context, question=prompt)
                        resp = llm.invoke(messages)
                        response = (getattr(resp, "content", "") or "").strip() or "1) ç­”æ¡ˆï¼šä¸çŸ¥é“\n2) å¼•ç”¨ï¼šæ— \n3) è¯æ®åŸæ–‡ï¼šæ— "

                response = fill_evidence_if_missing(response, docs)

                message_placeholder.markdown(response)

                with st.expander("æœ¬æ¬¡æ£€ç´¢åˆ°çš„ sourcesï¼ˆç”¨äºæ ¸å¯¹/æ’é”™ï¼‰", expanded=bool(learning_mode)):
                    if best_score is not None:
                        st.caption(
                            f"best_score={best_score} ({score_mode}), threshold={score_threshold}, "
                            f"k={top_k}, term_hits={hits}"
                        )
                    if learning_mode:
                        st.write(
                            {
                                "missing_field": missing or "",
                                "topic_missing": topic_missing,
                                "refuse_by_score": bool(refuse_by_score),
                                "extract_terms": extract_query_terms(prompt),
                            }
                        )
                        if scores:
                            st.write(
                                {
                                    "scores": scores,
                                    "score_mode": score_mode,
                                }
                            )
                    st.text(context or "(ç©º)")

                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                message_placeholder.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")


if __name__ == "__main__":
    load_env()
    render_page()
