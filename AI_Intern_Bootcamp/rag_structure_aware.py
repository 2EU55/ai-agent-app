import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
DEFAULT_DOC_PATH = "finance_data.md"  # é»˜è®¤ä½¿ç”¨æˆ‘ä»¬åˆšå‡†å¤‡çš„è´¢åŠ¡æ•°æ®

def build_embeddings(api_key: str):
    return OpenAIEmbeddings(
        model="BAAI/bge-m3",
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
    )

def build_llm(api_key: str):
    return ChatOpenAI(
        model="Qwen/Qwen2.5-7B-Instruct", # ä½¿ç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹
        api_key=api_key,
        base_url=SILICONFLOW_BASE_URL,
        temperature=0,
    )

def build_vectorstore_advanced(api_key: str, doc_path: str):
    """
    å…³é”®æ•™å­¦ç‚¹ï¼šç»“æ„åŒ–åˆ‡åˆ†
    ä¸å†ä¸€è‚¡è„‘åˆ‡ç¢ï¼Œè€Œæ˜¯å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼Œä¿ç•™ä¸Šä¸‹æ–‡å½’å±ã€‚
    """
    # 1. è¯»å–åŸå§‹å†…å®¹
    with open(doc_path, "r", encoding="utf-8") as f:
        text = f.read()

    # 2. ç¬¬ä¸€æ¬¡åˆ‡åˆ†ï¼šæŒ‰ Markdown æ ‡é¢˜ (ä¿ç•™ç»“æ„ä¿¡æ¯)
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_header_splits = markdown_splitter.split_text(text)

    # 3. ç¬¬äºŒæ¬¡åˆ‡åˆ†ï¼šåœ¨æ ‡é¢˜åˆ‡åˆ†çš„åŸºç¡€ä¸Šï¼Œå†æ§åˆ¶å­—ç¬¦é•¿åº¦ (é˜²æ­¢æŸä¸€æ®µå¤ªé•¿)
    # è¿™é‡Œçš„å…³é”®æ˜¯ï¼šMetadata ä¼šè¢«ç»§æ‰¿ï¼
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50
    )
    splits = text_splitter.split_documents(md_header_splits)

    # 4. å‘é‡åŒ–å­˜å‚¨
    embeddings = build_embeddings(api_key)
    return Chroma.from_documents(documents=splits, embedding=embeddings)

def render_page():
    st.set_page_config(page_title="RAG è¿›é˜¶ç‰ˆï¼šç»“æ„åŒ–æ„ŸçŸ¥", page_icon="ğŸ§ ")
    st.title("ğŸ§  RAG è¿›é˜¶æ•™å­¦ï¼šç»“æ„åŒ–æ–‡æ¡£æ£€ç´¢")
    st.caption("æ•™å­¦ç›®æ ‡ï¼šè§£å†³è¡¨æ ¼ä¸å±‚çº§æ–‡æ¡£çš„æ£€ç´¢éš¾é¢˜")

    api_key = os.getenv("SILICONFLOW_API_KEY") or st.text_input("API Key", type="password")
    
    if not api_key:
        st.warning("è¯·é…ç½® API Key")
        return

    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # è¾“å…¥æ¡†
    question = st.chat_input("è¯•ç€é—®ï¼šç«æ˜Ÿæ¢é™©çš„ç¥¨ä»·æ˜¯å¤šå°‘ï¼Ÿ")
    if question:
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.chat_message("user").markdown(question)
        st.session_state.messages.append({"role": "user", "content": question})

        # æ ¸å¿ƒé€»è¾‘
        with st.spinner("æ­£åœ¨è¿›è¡Œç»“æ„åŒ–æ£€ç´¢..."):
            vectorstore = build_vectorstore_advanced(api_key, DEFAULT_DOC_PATH)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            docs = retriever.invoke(question)
            
            # æ„å»ºå¢å¼ºåçš„ä¸Šä¸‹æ–‡ (æ˜¾å¼å±•ç¤ºå…ƒæ•°æ®)
            context_parts = []
            for d in docs:
                # å°†å…ƒæ•°æ®ä¸­çš„æ ‡é¢˜æ‹¼æ¥å›æ­£æ–‡ï¼Œè®© LLM çŸ¥é“è¿™æ®µè¯å±äºå“ªä¸ªç« èŠ‚
                header_path = " > ".join(filter(None, [
                    d.metadata.get("Header 1"),
                    d.metadata.get("Header 2"),
                    d.metadata.get("Header 3")
                ]))
                context_parts.append(f"ã€ç« èŠ‚ï¼š{header_path}ã€‘\n{d.page_content}")
            
            context = "\n\n".join(context_parts)
            
            # ç”Ÿæˆå›ç­”
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
            æ³¨æ„ï¼šä¸Šä¸‹æ–‡åŒ…å«ã€ç« èŠ‚ã€‘ä¿¡æ¯ï¼Œè¯·åˆ©ç”¨è¿™äº›å±‚çº§å…³ç³»æ¥å‡†ç¡®å®šä½ä¿¡æ¯ã€‚
            
            ä¸Šä¸‹æ–‡ï¼š
            {context}
            
            é—®é¢˜ï¼š{question}
            """
            
            llm = build_llm(api_key)
            response = llm.invoke(prompt)
            answer = response.content

        # æ˜¾ç¤ºå›ç­”
        st.chat_message("assistant").markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})
        
        # æ•™å­¦å±•ç¤ºåŒºï¼šè®©ç”¨æˆ·çœ‹åˆ°åº•å±‚çš„ä¸åŒ
        with st.expander("ğŸ” å¯¼å¸ˆè§†è§’ï¼šçœ‹çœ‹æˆ‘ä»¬æ£€ç´¢åˆ°äº†ä»€ä¹ˆï¼Ÿ"):
            st.markdown("### ç»“æ„åŒ–åˆ‡åˆ†çš„æ•ˆæœ")
            st.write("æ³¨æ„è§‚å¯Ÿæ¯ä¸ªç‰‡æ®µçš„ **ã€ç« èŠ‚ã€‘** æ ‡è®°ã€‚æ™®é€šåˆ‡åˆ†ä¼šä¸¢å¤±è¿™äº›ä¿¡æ¯ï¼Œå¯¼è‡´ LLM ä¸çŸ¥é“è¿™ä¸¤ä¸ªæ•°å­—å±äºå“ªä¸ªéƒ¨é—¨æˆ–å“ªä¸€å¹´ã€‚")
            for i, part in enumerate(context_parts, 1):
                st.info(f"**ç‰‡æ®µ {i}**\n\n{part}")

if __name__ == "__main__":
    render_page()
