import os
import json
import streamlit as st
from openai import OpenAI

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
DEFAULT_MODEL = "Qwen/Qwen2.5-7B-Instruct"

MAX_BUFFER_MESSAGES = 12
KEEP_MESSAGES_AFTER_SUMMARY = 6


def load_env() -> None:
    try:
        from dotenv import load_dotenv

        load_dotenv()
    except Exception:
        return


def get_api_key() -> str:
    return os.getenv("SILICONFLOW_API_KEY", "")


def get_state_file_path() -> str:
    base_dir = os.path.dirname(os.path.abspath(__file__))
    state_dir = os.path.join(base_dir, ".local_state")
    os.makedirs(state_dir, exist_ok=True)
    return os.path.join(state_dir, "memory_chatbot.json")


def load_persisted_state() -> dict:
    path = get_state_file_path()
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return data
    except Exception:
        return {}
    return {}


def save_persisted_state(chat_messages: list[dict[str, str]], memory_summary: str) -> None:
    path = get_state_file_path()
    data = {"chat_messages": chat_messages, "memory_summary": memory_summary}
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def init_state() -> None:
    persisted = load_persisted_state()
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = persisted.get("chat_messages", [])
    if "memory_summary" not in st.session_state:
        st.session_state.memory_summary = persisted.get("memory_summary", "")


def chat_once(client: OpenAI, model: str, messages: list[dict[str, str]]) -> str:
    resp = client.chat.completions.create(model=model, messages=messages, stream=False)
    return resp.choices[0].message.content or ""


def should_summarize(messages: list[dict[str, str]]) -> bool:
    return len(messages) > MAX_BUFFER_MESSAGES


def summarize_memory(client: OpenAI, model: str, old_summary: str, messages_to_summarize: list[dict[str, str]]) -> str:
    system = (
        "ä½ æ˜¯ä¸€ä¸ªâ€œå¯¹è¯è®°å¿†å‹ç¼©å™¨â€ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŠŠå¤šè½®å¯¹è¯å‹ç¼©æˆå¯æŒç»­ç´¯ç§¯çš„è®°å¿†æ‘˜è¦ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) ç”¨ä¸­æ–‡è¾“å‡ºã€‚\n"
        "2) åªä¿ç•™å¯¹æœªæ¥å¯¹è¯æœ‰ç”¨çš„ä¿¡æ¯ï¼šç”¨æˆ·çš„ç›®æ ‡ã€åå¥½ã€çº¦æŸã€å·²ç¡®è®¤äº‹å®ã€å·²åšçš„å†³å®šã€‚\n"
        "3) ä¸è¦è®°å½•æ— æ„ä¹‰å¯’æš„ã€‚\n"
        "4) è¾“å‡ºä¸ºä¸è¶…è¿‡ 10 æ¡è¦ç‚¹åˆ—è¡¨ï¼Œæ¯æ¡å°½é‡çŸ­ã€‚\n"
    )
    transcript = "\n".join([f"{m['role']}: {m['content']}" for m in messages_to_summarize])
    user = (
        f"æ—§çš„è®°å¿†æ‘˜è¦ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰ï¼š\n{old_summary}\n\n"
        f"éœ€è¦å‹ç¼©çš„å¯¹è¯ï¼š\n{transcript}\n\n"
        "è¯·è¾“å‡ºæ›´æ–°åçš„è®°å¿†æ‘˜è¦ï¼ˆè¦ç‚¹åˆ—è¡¨ï¼‰ã€‚"
    )
    return chat_once(client, model, [{"role": "system", "content": system}, {"role": "user", "content": user}])


def build_messages_for_model(
    memory_summary: str,
    buffer_messages: list[dict[str, str]],
    user_prompt: str,
) -> list[dict[str, str]]:
    system = (
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨ã€å‹å¥½çš„ AI åŠ©æ‰‹ã€‚\n"
        "ä½ ä¼šä¼˜å…ˆå‚è€ƒâ€œè®°å¿†æ‘˜è¦â€ï¼Œå†å‚è€ƒæœ€è¿‘å¯¹è¯ç¼“å†²åŒºã€‚\n"
        "å¦‚æœè®°å¿†æ‘˜è¦é‡Œæ²¡æœ‰ä¿¡æ¯ï¼Œä¸è¦èƒ¡ç¼–ï¼Œåº”è¯¥å‘ç”¨æˆ·è¿½é—®ã€‚\n"
    )
    memory_block = memory_summary.strip()
    if memory_block:
        system += f"\nè®°å¿†æ‘˜è¦ï¼š\n{memory_block}\n"
    messages = [{"role": "system", "content": system}]
    messages.extend(buffer_messages)
    messages.append({"role": "user", "content": user_prompt})
    return messages


def render() -> None:
    st.set_page_config(page_title="Day6 è®°å¿†èŠå¤©åŠ©æ‰‹", page_icon="ğŸ§ ", layout="wide")
    st.title("ğŸ§  Day 6ï¼šè®°å¿†èŠå¤©åŠ©æ‰‹ï¼ˆç¼“å†²è®°å¿† + æ€»ç»“å¼è®°å¿†ï¼‰")
    st.markdown("ç›®æ ‡ï¼šå­¦ä¼šâ€œè®© AI è®°ä½å…³é”®äº‹å®â€ï¼ŒåŒæ—¶æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆæˆæœ¬/å»¶è¿Ÿ/ç¨³å®šæ€§ï¼‰ã€‚")

    with st.sidebar:
        st.header("é…ç½®")
        api_key = st.text_input("API Key", value=get_api_key(), type="password")
        model = st.text_input("æ¨¡å‹", value=DEFAULT_MODEL)
        st.write(f"Base URLï¼š{SILICONFLOW_BASE_URL}")
        st.divider()
        st.subheader("å½“å‰è®°å¿†æ‘˜è¦")
        st.caption("å½“å¯¹è¯è¿‡é•¿æ—¶ï¼Œä¼šè‡ªåŠ¨æŠŠæ—§å¯¹è¯å‹ç¼©åˆ°è¿™é‡Œã€‚")
        st.code(st.session_state.get("memory_summary", "") or "(ç©º)")
        if st.button("æ¸…ç©ºè®°å¿†ä¸èŠå¤©è®°å½•", use_container_width=True):
            st.session_state.chat_messages = []
            st.session_state.memory_summary = ""
            save_persisted_state(chat_messages=[], memory_summary="")
            st.rerun()

    if not api_key:
        st.warning("å…ˆåœ¨å·¦ä¾§é…ç½® API Keyï¼ˆæ”¯æŒä» .env è‡ªåŠ¨è¯»å–ï¼‰ã€‚")
        return

    client = OpenAI(api_key=api_key, base_url=SILICONFLOW_BASE_URL)

    for msg in st.session_state.chat_messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("éšä¾¿èŠç‚¹ä»€ä¹ˆï¼ˆæ¯”å¦‚ï¼šæˆ‘å«å•¥ã€æˆ‘æƒ³å­¦ä»€ä¹ˆã€æˆ‘æœ‰å“ªäº›é¡¹ç›®ï¼‰"):
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        if should_summarize(st.session_state.chat_messages):
            keep = st.session_state.chat_messages[-KEEP_MESSAGES_AFTER_SUMMARY:]
            to_sum = st.session_state.chat_messages[:-KEEP_MESSAGES_AFTER_SUMMARY]
            st.session_state.memory_summary = summarize_memory(
                client=client,
                model=model,
                old_summary=st.session_state.memory_summary,
                messages_to_summarize=to_sum,
            )
            st.session_state.chat_messages = keep

        model_messages = build_messages_for_model(
            memory_summary=st.session_state.memory_summary,
            buffer_messages=st.session_state.chat_messages[-MAX_BUFFER_MESSAGES:],
            user_prompt=prompt,
        )

        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("æ€è€ƒä¸­â€¦")
            reply = chat_once(client, model, model_messages)
            placeholder.markdown(reply)

        st.session_state.chat_messages.append({"role": "assistant", "content": reply})
        save_persisted_state(
            chat_messages=st.session_state.chat_messages,
            memory_summary=st.session_state.memory_summary,
        )


load_env()
init_state()
render()
